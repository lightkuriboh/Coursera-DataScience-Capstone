---
title: "Exploratory Analysis on SwiftKey Dataset"
author: "kuriboh"
date: "10/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Normalizes text in the dataset  
The first step I want to perform is normalizing words in the datasets:  
* Transform words to lowercase  
* Remove all punctuation  
* Words are separated by only spaces  
```{r}
normalize_paragraph <- function(paragraph) {
    remove_non_alphabetical <- function(sentence) {
        answer <- ''
        for (ch in strsplit(sentence, '')[[1]]) {
            if (ch >= 'a' && ch <= 'z' || ch == ' ') {
                answer <- paste(answer, ch, sep='')
            }
        }
        answer
    }
    n_sentences <- length(paragraph)
    answer <- list()
    counter <- 1
    for (i in 1:n_sentences) {
        paragraph[i] <- tolower(paragraph[i])
        paragraph[i] <- remove_non_alphabetical(paragraph[i])
        words <- strsplit(paragraph[i], ' ')[[1]]
        indices <- sapply(words, function(word) {
            ans <- FALSE
            if (nchar(word) > 0) {
                ans <- TRUE
            }
            ans
        })
        if (class(indices) == 'logical') {
            answer[[counter]] <- words[indices]
            counter <- counter + 1
        }
    }
    answer
}
# my_paragraph <- c('A7, bU*  c', 'A78u. o  O')
# normalize_paragraph(my_paragraph)
# head(normalize_paragraph(news_data[1:1]))
news_data <- normalize_paragraph(news_data)
blogs_data <- normalize_paragraph(blogs_data)
twitter_data <- normalize_paragraph(twitter_data)
```
  
## Does exploratory analysis  
* Firstly I'm curious about how many sentences are there and how many words in each sentence are there in average  
```{r}
length(twitter_data)
mean(sapply(twitter_data, length))

length(blogs_data)
mean(sapply(blogs_data, length))

length(news_data)
mean(sapply(news_data, length))
```
  
## One more optional step, I consider text from those three datasets has the same role. Thus I will merge those dataset into one  
```{r}
combined_paragraph <- do.call(c, list(blogs_data, news_data, twitter_data))
```
  
## Get single words occurences  
```{r}
library(hash)
dict <- hash()
increase_count <- function (my_key) {
    separator <- ' '
    my_key <- paste(my_key, collapse=separator)
    if (has.key(my_key, dict)) {
        dict[[my_key]] <- dict[[my_key]] + 1
    } else {
        dict[[my_key]] <- 1
    }
}

for (sentence in combined_paragraph) {
    for (word in sentence) {
        increase_count(word)
    }
}

one_gram_distribution <- unlist(as.list(dict))
one_gram_distribution <- order(one_gram_distribution, decreasing=TRUE)
head(one_gram_distribution)
```
