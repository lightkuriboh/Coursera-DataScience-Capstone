---
title: "NGramModel"
author: "kuriboh"
date: "10/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_chunk$set(cache=TRUE)
```
  
## Load data into memory and does some summary on the three datasets    
```{r}
twitter_conn <- file('data/en_US/en_US.twitter.txt')
news_conn <- file('data/en_US/en_US.news.txt')
blogs_conn <- file('data/en_US/en_US.blogs.txt')

open(twitter_conn)
open(news_conn)
open(blogs_conn)

twitter_data <- readLines(twitter_conn, skipNul=TRUE)
blogs_data <- readLines(blogs_conn, skipNul=TRUE)
news_data <- readLines(news_conn, skipNul=TRUE)

close(twitter_conn)
close(blogs_conn)
close(news_conn)
```
  
## Setting seed, loading required libraries  
```{r, message=FALSE}
library(tm)
library(dplyr)
library(wordcloud)
library(ggplot2)
library(data.table)
library(RWeka)
library(markovchain)

set.seed(210199)
```
  
## One more optional step, I consider text from those three datasets has the same role. Thus I will merge those dataset into one  
```{r}
combined_paragraph <- do.call(c, list(blogs_data, news_data, twitter_data))
```
  
## Since the data size is large (100M words), I will extract 0.5% of the dataset  
```{r}
select_proportion <- 0.00075
combined_paragraph <- combined_paragraph[rbinom(n=length(combined_paragraph), size=1, prob=select_proportion) == 1]
```
  
## Normalizes text in the dataset  
The first step I want to perform is normalizing words in the datasets:  
* Transform words to lowercase  
* Remove all punctuation  
* Words are separated by only spaces  
```{r}
replacePunctuation <- tm::content_transformer(function(x) gsub("[^[:alnum:][:space:]'`]", " ", x))
clean_corpus <- function(corpus) {
    corpus %>%
        tm::tm_map(tm::stripWhitespace) %>%
        tm::tm_map(replacePunctuation) %>%
        tm::tm_map(tm::removeNumbers) %>%
        tm::tm_map(content_transformer(tolower))
}

combined_paragraph <- tm::VCorpus(tm::VectorSource(combined_paragraph))
clean_corpus(combined_paragraph)
```
  
## Making training set and test set with ratio 3-7   
```{r}
test_fraction <- 0.3
chosen_indices <- rbinom(n=length(combined_paragraph), size=1, prob=test_fraction)
train_data <- combined_paragraph[chosen_indices == 0]
test_data <- combined_paragraph[chosen_indices == 1]
```
  
## Removing "bad" words  
I found [this project](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words) which lists bad words in many languages  
```{r}
load_bad_words <- function(bad_words_url) {
    connection <- url(bad_words_url)
    lines <- readLines(connection)
    close(connection)
    lines
}

bad_words <- load_bad_words(
    "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
)

pretty_train_data <- tm_map(train_data, removeWords, bad_words)
```
  
## Does exploratory analysis  
* Making a function to present N-Gram  
```{r}
create_ngrams_data_frame <- function (document_term_matrix) {
    frequencies <- colSums(document_term_matrix)
    ngrams <- data.frame(ngram = names(frequencies), frequency=frequencies, stringsAsFactors=FALSE)
    ngrams <- arrange(ngrams, desc(frequency))
    rownames(ngrams) <- 1:length(frequencies)
    ngrams
}

making_document_term_matrix <- function (n_gram) {
    n_gram_tokenizer <- function(x) {
        NGramTokenizer(x, Weka_control(min=n_gram, max=n_gram))
    }
    DocumentTermMatrix(pretty_train_data,
                       control = list(
                           tokenize=n_gram_tokenizer,
                           wordLengths=c(1, Inf)
                           )
                       )
}
```
  
Getting N-Gram distribution of size from One to Four  
* 1-Gram  
```{r}
one_grams <- create_ngrams_data_frame(as.matrix(making_document_term_matrix(1)))
one_grams[1:50, ]
```  

* 2-Gram  
```{r}
two_grams <- create_ngrams_data_frame(as.matrix(making_document_term_matrix(2)))
two_grams[1:50, ]
```  

* 3-Gram  
```{r}
three_grams <- create_ngrams_data_frame(as.matrix(making_document_term_matrix(3)))
three_grams[1:50, ]
```  

* 4-Gram  
```{r}
four_grams <- create_ngrams_data_frame(as.matrix(making_document_term_matrix(4)))
four_grams[1:50, ]
```  
  
# N-Gram plots  
```{r}
draw_ngram_plot <- function (ngrams) {
    ggplot(ngrams, aes(x=ngram, y=frequency, fill=ngram)) +
        geom_bar(stat='identity') +
        scale_x_discrete(limits=ngrams$ngram) +
        ggtitle(paste(length(strsplit(ngrams[1]$ngram, ' ')[[1]]), '-Grams: ', 'Top', nrow(ngrams), 'word Frequencies in the corpus')) +
        ylab('Frequency') +
        theme(axis.text.x=element_text(size=12, angle=90, hjust=1, vjust=0.5)) +
        theme(legend.position="none")
}
words_cloud <- function(ngrams) {
    wordcloud(ngrams$ngram, ngrams$frequency, scale = c(4, 0.5), colors = brewer.pal(8, "Dark2"))
}

top_one_grams <- one_grams[1:50, ]
top_two_grams <- two_grams[1:50, ]
top_three_grams <- three_grams[1:50, ]
top_four_grams <- four_grams[1:50, ]

draw_ngram_plot(top_one_grams)
draw_ngram_plot(top_two_grams)
draw_ngram_plot(top_three_grams)
draw_ngram_plot(top_four_grams)

words_cloud(top_one_grams)
words_cloud(top_two_grams)
words_cloud(top_three_grams)
words_cloud(top_four_grams)
# save(one_grams, two_grams, three_grams, four_grams, file='ngrams_data.RData')
```

  